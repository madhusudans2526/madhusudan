File path: /home/hadoop/prac/spark/requirement/json_file_new.json

{"claims":[{"id":"100","name":"AAA","salary":"10000","termination_date":"20200803","claim1":{"location":"HYD","deptno":10}},{"id":"101","name":"BBB","salary":"11000","claim1":{"location":"CHN","deptno":20}},{"id":"102","name":"CCC","salary":"10400","termination_date":"20200803","claim1":{"location":"BAN","deptno":20}},{"id":"103","name":"DDD","salary":"","claim1":{"location":"CHN","deptno":30}},{"id":"104","name":"EEE","salary":"10600","termination_date":"20200803","claim1":{"location":"HYD","deptno":20}},{"id":"105","name":"AAA","salary":"10000","claim1":{"location":"BAN","deptno":20}},{"id":"106","name":"BBB","salary":"11000","claim1":{"location":"HYD","deptno":30}},{"id":"107","name":"CCC","salary":"10400","claim1":{"location":"CHN","deptno":30}},{"id":"108","name":"AAA","salary":"10000","claim1":{"location":"BAN","deptno":30}},{"id":"109","name":"","salary":"10800","termination_date":"20200803","claim1":{"location":"HYD","deptno":10}},{"id":"110","name":"","salary":"10000","claim1":{"location":"BAN","deptno":10}},{"id":"111","name":"CCC","salary":"","claim1":{"location":"HYD","deptno":40}},{"id":"112","name":"AAA","salary":"","termination_date":"20200803","claim1":{"location":"BAN","deptno":40}},{"id":"113","name":"BBB","salary":"11000","termination_date":"20200803","claim1":{"location":"CHN","deptno":40}},{"id":"114","name":"CCC","salary":"","claim1":{"location":"HYD","deptno":10}},{"id":"115","name":"","salary":"10000","claim1":{"location":"BAN","deptno":20}},{"id":"116","name":"","salary":"11000","claim1":{"location":"CHN","deptno":30}},{"id":"117","name":"CCC","salary":"10400","claim1":{"location":"HYD","deptno":10}}]}

-----------------------------------------------------------------------------------

scala> val new_file = spark.read.option("multiLines","true").json("file:///home/hadoop/prac/spark/requirement/json_file_new.json")
new_file: org.apache.spark.sql.DataFrame = [claims: array<struct<claim1:struct<deptno:bigint,location:string>,id:string,name:string,salary:string,termination_date:string>>]

scala> new_file.printSchema
root
 |-- claims: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- claim1: struct (nullable = true)
 |    |    |    |-- deptno: long (nullable = true)
 |    |    |    |-- location: string (nullable = true)
 |    |    |-- id: string (nullable = true)
 |    |    |-- name: string (nullable = true)
 |    |    |-- salary: string (nullable = true)
 |    |    |-- termination_date: string (nullable = true)


scala> new_file.show
+--------------------+
|              claims|
+--------------------+
|[[[10,HYD],100,AA...|
+--------------------+

scala> val zip = udf((id: Seq[String], name: Seq[String], salary: Seq[String], termination_date: Seq[String], deptno: Seq[Long], location: Seq[String]) => id.zip(name).zip(salary).zip(termination_date).zip(deptno).zip(location))
zip: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function6>,ArrayType(StructType(StructField(_1,StructType(StructField(_1,StructType(StructField(_1,StructType(StructField(_1,StructType(StructField(_1,StringType,true), StructField(_2,StringType,true)),true), StructField(_2,StringType,true)),true), StructField(_2,StringType,true)),true), StructField(_2,LongType,false)),true), StructField(_2,StringType,true)),true),Some(List(ArrayType(StringType,true), ArrayType(StringType,true), ArrayType(StringType,true), ArrayType(StringType,true), ArrayType(LongType,false), ArrayType(StringType,true))))

scala> 

scala> val new_cols = new_file.withColumn("new_claims", explode(zip($"claims.id",$"claims.name",$"claims.salary",$"claims.termination_date",$"claims.claim1.deptno",$"claims.claim1.location")))
new_cols: org.apache.spark.sql.DataFrame = [claims: array<struct<claim1:struct<deptno:bigint,location:string>,id:string,name:string,salary:string,termination_date:string>>, new_claims: struct<_1: struct<_1: struct<_1: struct<_1: struct<_1: string, _2: string>, _2: string>, _2: string>, _2: bigint>, _2: string>]

scala> 

scala> new_cols.printSchema
root
 |-- claims: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- claim1: struct (nullable = true)
 |    |    |    |-- deptno: long (nullable = true)
 |    |    |    |-- location: string (nullable = true)
 |    |    |-- id: string (nullable = true)
 |    |    |-- name: string (nullable = true)
 |    |    |-- salary: string (nullable = true)
 |    |    |-- termination_date: string (nullable = true)
 |-- new_claims: struct (nullable = true)
 |    |-- _1: struct (nullable = true)
 |    |    |-- _1: struct (nullable = true)
 |    |    |    |-- _1: struct (nullable = true)
 |    |    |    |    |-- _1: struct (nullable = true)
 |    |    |    |    |    |-- _1: string (nullable = true)
 |    |    |    |    |    |-- _2: string (nullable = true)
 |    |    |    |    |-- _2: string (nullable = true)
 |    |    |    |-- _2: string (nullable = true)
 |    |    |-- _2: long (nullable = false)
 |    |-- _2: string (nullable = true)


scala> val final_claims = new_cols.select($"new_claims._1._1._1._1._1".as("id"),$"new_claims._1._1._1._1._2".as("name"),$"new_claims._1._1._1._2".as("salary"),$"new_claims._1._1._2".as("termination_date"),$"new_claims._1._2".as("deptno"),$"new_claims._2".as("location")).drop("claims")
final_claims: org.apache.spark.sql.DataFrame = [id: string, name: string ... 4 more fields]


scala> final_claims.printSchema
root
 |-- id: string (nullable = true)
 |-- name: string (nullable = true)
 |-- salary: string (nullable = true)
 |-- termination_date: string (nullable = true)
 |-- deptno: long (nullable = true)
 |-- location: string (nullable = true)


scala> final_claims.show
+---+----+------+----------------+------+--------+
| id|name|salary|termination_date|deptno|location|
+---+----+------+----------------+------+--------+
|100| AAA| 10000|        20200803|    10|     HYD|
|101| BBB| 11000|            null|    20|     CHN|
|102| CCC| 10400|        20200803|    20|     BAN|
|103| DDD|      |            null|    30|     CHN|
|104| EEE| 10600|        20200803|    20|     HYD|
|105| AAA| 10000|            null|    20|     BAN|
|106| BBB| 11000|            null|    30|     HYD|
|107| CCC| 10400|            null|    30|     CHN|
|108| AAA| 10000|            null|    30|     BAN|
|109|    | 10800|        20200803|    10|     HYD|
|110|    | 10000|            null|    10|     BAN|
|111| CCC|      |            null|    40|     HYD|
|112| AAA|      |        20200803|    40|     BAN|
|113| BBB| 11000|        20200803|    40|     CHN|
|114| CCC|      |            null|    10|     HYD|
|115|    | 10000|            null|    20|     BAN|
|116|    | 11000|            null|    30|     CHN|
|117| CCC| 10400|            null|    10|     HYD|
+---+----+------+----------------+------+--------+


scala> 

------------------------------------------------------------------------------------------------------------------


val new_file = spark.read.option("multiLines","true").json("file:///home/hadoop/prac/spark/requirement/json_file_new.json")
new_file.printSchema
new_file.show
val zip = udf((id: Seq[String], name: Seq[String], salary: Seq[String], termination_date: Seq[String], deptno: Seq[Long], location: Seq[String]) => id.zip(name).zip(salary).zip(termination_date).zip(deptno).zip(location))
val new_cols = new_file.withColumn("new_claims", explode(zip($"claims.id",$"claims.name",$"claims.salary",$"claims.termination_date",$"claims.claim1.deptno",$"claims.claim1.location")))
new_cols.printSchema
val final_claims = new_cols.select($"new_claims._1._1._1._1._1".as("id"),$"new_claims._1._1._1._1._2".as("name"),$"new_claims._1._1._1._2".as("salary"),$"new_claims._1._1._2".as("termination_date"),$"new_claims._1._2".as("deptno"),$"new_claims._2".as("location")).drop("claims")
final_claims.printSchema
final_claims.show
